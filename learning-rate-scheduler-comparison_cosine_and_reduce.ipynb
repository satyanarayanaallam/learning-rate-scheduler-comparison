{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31240,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from torchvision import datasets, transforms","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-25T21:07:40.101364Z","iopub.execute_input":"2025-12-25T21:07:40.101557Z","iopub.status.idle":"2025-12-25T21:07:49.507504Z","shell.execute_reply.started":"2025-12-25T21:07:40.101535Z","shell.execute_reply":"2025-12-25T21:07:49.506933Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Step 1: Get the Data\ntransform = transforms.Compose([\n transforms.ToTensor(),\n transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))    \n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T21:08:20.772421Z","iopub.execute_input":"2025-12-25T21:08:20.772994Z","iopub.status.idle":"2025-12-25T21:08:20.776842Z","shell.execute_reply.started":"2025-12-25T21:08:20.772968Z","shell.execute_reply":"2025-12-25T21:08:20.776171Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Load dataset\ntrain_dataset = datasets.CIFAR10(root='./data',train=True,download=True,transform=transform)\ntest_dataset = datasets.CIFAR10(root='./data',train=False,download=True,transform=transform)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T21:08:27.172632Z","iopub.execute_input":"2025-12-25T21:08:27.172931Z","iopub.status.idle":"2025-12-25T21:08:32.616456Z","shell.execute_reply.started":"2025-12-25T21:08:27.172910Z","shell.execute_reply":"2025-12-25T21:08:32.615888Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 170M/170M [00:02<00:00, 82.9MB/s] \n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from torch.utils.data import random_split,DataLoader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T21:08:34.688096Z","iopub.execute_input":"2025-12-25T21:08:34.688432Z","iopub.status.idle":"2025-12-25T21:08:34.693135Z","shell.execute_reply.started":"2025-12-25T21:08:34.688406Z","shell.execute_reply":"2025-12-25T21:08:34.692089Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"test_size = int(0.5 * len(test_dataset))   # 50% for testing\nval_size = int(0.5 * len(test_dataset))  # 50% for validation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T21:08:37.001299Z","iopub.execute_input":"2025-12-25T21:08:37.001653Z","iopub.status.idle":"2025-12-25T21:08:37.005897Z","shell.execute_reply.started":"2025-12-25T21:08:37.001628Z","shell.execute_reply":"2025-12-25T21:08:37.005043Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Dividing test data to test data and val data\ntest_dataset, val_dataset = random_split(test_dataset, [test_size, val_size])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T21:09:06.519857Z","iopub.execute_input":"2025-12-25T21:09:06.520160Z","iopub.status.idle":"2025-12-25T21:09:06.526841Z","shell.execute_reply.started":"2025-12-25T21:09:06.520138Z","shell.execute_reply":"2025-12-25T21:09:06.526194Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"train_data_loader = DataLoader(train_dataset,batch_size=32,shuffle=True,num_workers=4)# change number of workers to 4 when we ran in cuda env\ntest_data_loader = DataLoader(test_dataset,batch_size=32,shuffle=False,num_workers=4)\nval_data_loader = DataLoader(val_dataset,batch_size=32,shuffle=False,num_workers=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T21:09:08.354402Z","iopub.execute_input":"2025-12-25T21:09:08.355171Z","iopub.status.idle":"2025-12-25T21:09:08.359686Z","shell.execute_reply.started":"2025-12-25T21:09:08.355142Z","shell.execute_reply":"2025-12-25T21:09:08.358771Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Model Creation\nimport torchvision.models as models\nimport torch\n\nmodel = models.resnet18(weights=True)  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T21:09:27.209596Z","iopub.execute_input":"2025-12-25T21:09:27.209875Z","iopub.status.idle":"2025-12-25T21:09:27.408381Z","shell.execute_reply.started":"2025-12-25T21:09:27.209856Z","shell.execute_reply":"2025-12-25T21:09:27.407565Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import torch.nn as nn\n\nnum_features = model.fc.in_features\nmodel.fc = nn.Linear(num_features, 10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T21:09:29.996632Z","iopub.execute_input":"2025-12-25T21:09:29.997023Z","iopub.status.idle":"2025-12-25T21:09:30.001440Z","shell.execute_reply.started":"2025-12-25T21:09:29.996991Z","shell.execute_reply":"2025-12-25T21:09:30.000734Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Make Infra ready by setting loss,optinizer and scheduler\nimport torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(),lr=0.001)\n# Scheduler: ReduceLROnPlateau\n# - mode='min' → expects validation loss to decrease\n# - factor=0.1 → multiply LR by 0.1 when triggered\n# - patience=3 → wait 3 epochs with no improvement before reducing LR\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer,\n    mode='min',\n    factor=0.1,\n    patience=3,\n    verbose=True\n)\n\nnum_epochs = 30\n\n# Move to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \n                      \"mps\" if torch.backends.mps.is_available() else \n                      \"cpu\")\n\nprint(\"Using device:\", device)\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T21:09:34.970707Z","iopub.execute_input":"2025-12-25T21:09:34.971277Z","iopub.status.idle":"2025-12-25T21:09:35.260013Z","shell.execute_reply.started":"2025-12-25T21:09:34.971253Z","shell.execute_reply":"2025-12-25T21:09:35.259369Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=10, bias=True)\n)"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"import os\n\n# Make sure the checkpoints directory exists\nos.makedirs(\"checkpoints\", exist_ok=True)\n\nbest_acc = 0.0\nfor epoch in range(num_epochs):  \n    model.train()\n    running_loss = 0.0\n    for images, labels in train_data_loader:\n        images, labels = images.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss+=loss.item()\n    \n    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n    model.eval()\n    val_loss = 0\n    correct, total = 0, 0\n    with torch.no_grad():\n        for images, labels in val_data_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            _, preds = torch.max(outputs, 1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n\n            val_loss+=loss         \n    acc = 100 * correct / total\n    print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")\n\n    avg_val_loss = val_loss / len(val_data_loader)\n    \n    # Step the scheduler with validation loss\n    scheduler.step(avg_val_loss)\n    \n    print(f\"Epoch {epoch+1}, Current LR: {scheduler.optimizer.param_groups[0]['lr']}, \"\n          f\"Val Loss: {avg_val_loss:.4f}\")\n\n    # --- Checkpointing ---\n    checkpoint = {\n        \"epoch\": epoch + 1,\n        \"model_state_dict\": model.state_dict(),\n        \"optimizer_state_dict\": optimizer.state_dict(),\n        \"accuracy\": acc,\n        \"loss\": loss.item()\n    }\n    torch.save(checkpoint, f\"checkpoints/epoch_{epoch+1}.pth\")\n    \n    # Save best model separately\n    if acc > best_acc:\n        best_acc = acc\n        torch.save(model.state_dict(), \"checkpoints/best_model.pth\")\n        print(\"Best model updated and saved!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T21:09:43.801166Z","iopub.execute_input":"2025-12-25T21:09:43.801771Z","iopub.status.idle":"2025-12-25T21:26:31.259198Z","shell.execute_reply.started":"2025-12-25T21:09:43.801746Z","shell.execute_reply":"2025-12-25T21:26:31.258081Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 0.6815\nValidation Accuracy: 72.88%\nEpoch 1, Current LR: 0.001, Val Loss: 0.8087\nBest model updated and saved!\nEpoch 2, Loss: 0.9311\nValidation Accuracy: 73.96%\nEpoch 2, Current LR: 0.001, Val Loss: 0.7977\nBest model updated and saved!\nEpoch 3, Loss: 1.2481\nValidation Accuracy: 75.26%\nEpoch 3, Current LR: 0.001, Val Loss: 0.7722\nBest model updated and saved!\nEpoch 4, Loss: 0.2942\nValidation Accuracy: 78.32%\nEpoch 4, Current LR: 0.001, Val Loss: 0.6592\nBest model updated and saved!\nEpoch 5, Loss: 0.3481\nValidation Accuracy: 75.04%\nEpoch 5, Current LR: 0.001, Val Loss: 1.1445\nEpoch 6, Loss: 0.3048\nValidation Accuracy: 80.52%\nEpoch 6, Current LR: 0.001, Val Loss: 0.6069\nBest model updated and saved!\nEpoch 7, Loss: 0.0711\nValidation Accuracy: 80.50%\nEpoch 7, Current LR: 0.001, Val Loss: 0.6495\nEpoch 8, Loss: 0.3039\nValidation Accuracy: 80.20%\nEpoch 8, Current LR: 0.001, Val Loss: 0.6917\nEpoch 9, Loss: 0.4269\nValidation Accuracy: 80.20%\nEpoch 9, Current LR: 0.001, Val Loss: 0.7034\nEpoch 10, Loss: 0.7101\nValidation Accuracy: 79.52%\nEpoch 10, Current LR: 0.0001, Val Loss: 0.7880\nEpoch 11, Loss: 0.0035\nValidation Accuracy: 82.08%\nEpoch 11, Current LR: 0.0001, Val Loss: 0.7078\nBest model updated and saved!\nEpoch 12, Loss: 0.0441\nValidation Accuracy: 82.64%\nEpoch 12, Current LR: 0.0001, Val Loss: 0.7821\nBest model updated and saved!\nEpoch 13, Loss: 0.0009\nValidation Accuracy: 82.30%\nEpoch 13, Current LR: 0.0001, Val Loss: 0.8567\nEpoch 14, Loss: 0.0005\nValidation Accuracy: 82.28%\nEpoch 14, Current LR: 1e-05, Val Loss: 0.9197\nEpoch 15, Loss: 0.0007\nValidation Accuracy: 82.06%\nEpoch 15, Current LR: 1e-05, Val Loss: 0.9340\nEpoch 16, Loss: 0.0005\nValidation Accuracy: 82.56%\nEpoch 16, Current LR: 1e-05, Val Loss: 0.9413\nEpoch 17, Loss: 0.0015\nValidation Accuracy: 82.18%\nEpoch 17, Current LR: 1e-05, Val Loss: 0.9584\nEpoch 18, Loss: 0.0563\nValidation Accuracy: 82.26%\nEpoch 18, Current LR: 1.0000000000000002e-06, Val Loss: 0.9758\nEpoch 19, Loss: 0.0007\nValidation Accuracy: 82.18%\nEpoch 19, Current LR: 1.0000000000000002e-06, Val Loss: 0.9770\nEpoch 20, Loss: 0.0073\nValidation Accuracy: 82.62%\nEpoch 20, Current LR: 1.0000000000000002e-06, Val Loss: 0.9559\nValidation Accuracy: 82.54%\nEpoch 24, Current LR: 1.0000000000000002e-07, Val Loss: 0.9663\nEpoch 25, Loss: 0.0077\nValidation Accuracy: 82.34%\nEpoch 25, Current LR: 1.0000000000000002e-07, Val Loss: 0.9839\nEpoch 26, Loss: 0.0003\nValidation Accuracy: 82.12%\nEpoch 26, Current LR: 1.0000000000000004e-08, Val Loss: 0.9882\nEpoch 27, Loss: 0.0055\nValidation Accuracy: 82.36%\nEpoch 27, Current LR: 1.0000000000000004e-08, Val Loss: 0.9632\nEpoch 28, Loss: 0.0101\nValidation Accuracy: 82.44%\nEpoch 28, Current LR: 1.0000000000000004e-08, Val Loss: 0.9574\nEpoch 29, Loss: 0.0001\nValidation Accuracy: 82.24%\nEpoch 29, Current LR: 1.0000000000000004e-08, Val Loss: 0.9708\nEpoch 30, Loss: 0.0004\nValidation Accuracy: 82.16%\nEpoch 30, Current LR: 1.0000000000000004e-08, Val Loss: 0.9996\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import torch.optim as optim\n\n# Optimizer (same as before)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Scheduler: CosineAnnealingLR\n# - T_max = number of epochs (or cycle length)\n# - eta_min = minimum LR at the end of the cosine curve\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n    optimizer,\n    T_max=30,       # full training run length\n    eta_min=1e-6    # minimum LR\n)\n\n# Training loop snippet\nfor epoch in range(30):\n    model.train()\n    running_loss = 0.0\n    \n    for images, labels in train_data_loader:\n        images, labels = images.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n    \n    # Validation step\n    model.eval()\n    val_loss = 0.0\n    correct, total = 0, 0\n    with torch.no_grad():\n        for images, labels in val_data_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n            \n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n    val_accuracy = 100 * correct / total\n    \n    # Step the scheduler\n    scheduler.step()\n    \n    print(f\"Epoch {epoch+1}, LR: {scheduler.get_last_lr()}, \"\n          f\"Train Loss: {running_loss/len(train_data_loader):.4f}, \"\n          f\"Val Loss: {val_loss/len(val_data_loader):.4f}, \"\n          f\"Val Acc: {val_accuracy:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T21:33:58.280725Z","iopub.execute_input":"2025-12-25T21:33:58.281276Z","iopub.status.idle":"2025-12-25T21:50:39.822178Z","shell.execute_reply.started":"2025-12-25T21:33:58.281247Z","shell.execute_reply":"2025-12-25T21:50:39.821408Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, LR: [0.0009972636867364526], Train Loss: 0.1263, Val Loss: 0.8491, Val Acc: 80.00%\nEpoch 2, LR: [0.000989084726566536], Train Loss: 0.1069, Val Loss: 0.8544, Val Acc: 79.20%\nEpoch 3, LR: [0.0009755527298894295], Train Loss: 0.0952, Val Loss: 0.9939, Val Acc: 77.34%\nEpoch 4, LR: [0.0009568159560924792], Train Loss: 0.0881, Val Loss: 0.8727, Val Acc: 79.50%\nEpoch 5, LR: [0.0009330796891903274], Train Loss: 0.0793, Val Loss: 0.9362, Val Acc: 79.56%\nEpoch 6, LR: [0.0009046039886902865], Train Loss: 0.0680, Val Loss: 0.9701, Val Acc: 78.24%\nEpoch 7, LR: [0.0008717008403259585], Train Loss: 0.0662, Val Loss: 0.9020, Val Acc: 80.30%\nEpoch 8, LR: [0.0008347307378762498], Train Loss: 0.0551, Val Loss: 0.9419, Val Acc: 80.30%\nEpoch 9, LR: [0.0007940987335200904], Train Loss: 0.0522, Val Loss: 0.9360, Val Acc: 79.84%\nEpoch 10, LR: [0.00075025], Train Loss: 0.0403, Val Loss: 1.0679, Val Acc: 78.74%\nEpoch 11, LR: [0.0007036649532163624], Train Loss: 0.0371, Val Loss: 1.0568, Val Acc: 79.94%\nEpoch 12, LR: [0.0006548539886902864], Train Loss: 0.0337, Val Loss: 1.1517, Val Acc: 79.86%\nEpoch 13, LR: [0.0006043518895634709], Train Loss: 0.0314, Val Loss: 1.0283, Val Acc: 80.52%\nEpoch 14, LR: [0.0005527119674021931], Train Loss: 0.0242, Val Loss: 1.1020, Val Acc: 80.06%\nEpoch 15, LR: [0.0005005000000000002], Train Loss: 0.0174, Val Loss: 1.1105, Val Acc: 80.48%\nEpoch 16, LR: [0.00044828803259780724], Train Loss: 0.0172, Val Loss: 1.1048, Val Acc: 80.78%\nEpoch 17, LR: [0.0003966481104365292], Train Loss: 0.0118, Val Loss: 1.0943, Val Acc: 81.22%\nEpoch 18, LR: [0.00034614601130971383], Train Loss: 0.0090, Val Loss: 1.1464, Val Acc: 81.22%\nEpoch 19, LR: [0.0002973350467836379], Train Loss: 0.0074, Val Loss: 1.1763, Val Acc: 80.86%\nEpoch 20, LR: [0.00025075000000000016], Train Loss: 0.0056, Val Loss: 1.1607, Val Acc: 81.34%\nEpoch 21, LR: [0.00020690126647990976], Train Loss: 0.0032, Val Loss: 1.2069, Val Acc: 81.24%\nEpoch 22, LR: [0.0001662692621237505], Train Loss: 0.0023, Val Loss: 1.2202, Val Acc: 82.12%\nEpoch 23, LR: [0.0001292991596740417], Train Loss: 0.0012, Val Loss: 1.2001, Val Acc: 82.08%\nEpoch 24, LR: [9.639601130971383e-05], Train Loss: 0.0009, Val Loss: 1.2042, Val Acc: 81.64%\nEpoch 25, LR: [6.792031080967288e-05], Train Loss: 0.0004, Val Loss: 1.2382, Val Acc: 82.06%\nEpoch 26, LR: [4.418404390752082e-05], Train Loss: 0.0004, Val Loss: 1.2510, Val Acc: 82.12%\nEpoch 27, LR: [2.5447270110570814e-05], Train Loss: 0.0004, Val Loss: 1.2589, Val Acc: 82.16%\nEpoch 28, LR: [1.1915273433464061e-05], Train Loss: 0.0003, Val Loss: 1.2716, Val Acc: 81.88%\nEpoch 29, LR: [3.736313263547436e-06], Train Loss: 0.0004, Val Loss: 1.2512, Val Acc: 82.16%\nEpoch 30, LR: [1e-06], Train Loss: 0.0003, Val Loss: 1.2759, Val Acc: 82.16%\n","output_type":"stream"}],"execution_count":16}]}