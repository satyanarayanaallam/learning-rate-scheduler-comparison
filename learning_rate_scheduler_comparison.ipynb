{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5398da5-3c72-496b-bdaa-0dec08c6713f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a898415-7810-494f-abba-9879a073876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get the Data\n",
    "transform = transforms.Compose([\n",
    " transforms.ToTensor(),\n",
    " transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a34ab871-da6b-4483-98b3-6756e4c79b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "train_dataset = datasets.CIFAR10(root='./data',train=True,download=True,transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data',train=False,download=True,transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c27429f-b423-4c2a-bbb0-8cc3dc64ec0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "715e5d2b-06cc-4355-8a31-86fa23c9a941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a46de84-49a8-40d1-abad-ca72d0c8e73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "479ed8d2-807d-4d66-a53c-26c616375149",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = int(0.5 * len(test_dataset))   # 50% for testing\n",
    "val_size = int(0.5 * len(test_dataset))  # 50% for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "353f399c-6060-4daa-ae23-7a037845015e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing test data to test data and val data\n",
    "test_dataset, val_dataset = random_split(test_dataset, [test_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5aebe71b-fef4-49f3-843a-294014614966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f692cff-ea57-4105-98de-e6949c526703",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(train_dataset,batch_size=32,shuffle=True,num_workers=0)# change number of workers to 4 when we ran in cuda env\n",
    "test_data_loader = DataLoader(test_dataset,batch_size=32,shuffle=False,num_workers=0)\n",
    "val_data_loader = DataLoader(val_dataset,batch_size=32,shuffle=False,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "daaf0c98-ae76-41dc-995e-f825bfb7eb46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Creation\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "\n",
    "model = models.resnet18(weights=None)  # no auto-download\n",
    "state_dict = torch.load(\"../pytorch-dataloading/resnet18-5c106cde.pth\",weights_only=False)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ed455d9-d55d-4c6c-a38f-6014b71b9483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74f995d3-bbaa-4396-b070-dce2d35eb1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make Infra ready by setting loss,optinizer and scheduler\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "# Scheduler: StepLR\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "num_epochs = 30\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                      \"mps\" if torch.backends.mps.is_available() else \n",
    "                      \"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7fd4bf8d-4557-4b1b-88b4-61f85528dd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.9107\n",
      "Validation Accuracy: 72.46%\n",
      "Best model updated and saved!\n",
      "Epoch 1, LR: [0.001], Train Loss: 1.0723, Val Loss: 0.8040, Val Acc: 72.46%\n",
      "Epoch 2, Loss: 0.7060\n",
      "Validation Accuracy: 76.80%\n",
      "Best model updated and saved!\n",
      "Epoch 2, LR: [0.001], Train Loss: 0.7545, Val Loss: 0.6923, Val Acc: 76.80%\n",
      "Epoch 3, Loss: 0.4130\n",
      "Validation Accuracy: 79.14%\n",
      "Best model updated and saved!\n",
      "Epoch 3, LR: [0.001], Train Loss: 0.6085, Val Loss: 0.6233, Val Acc: 79.14%\n",
      "Epoch 4, Loss: 0.4066\n",
      "Validation Accuracy: 79.98%\n",
      "Best model updated and saved!\n",
      "Epoch 4, LR: [0.001], Train Loss: 0.4935, Val Loss: 0.5882, Val Acc: 79.98%\n",
      "Epoch 5, Loss: 1.1649\n",
      "Validation Accuracy: 77.72%\n",
      "Epoch 5, LR: [0.001], Train Loss: 0.3949, Val Loss: 0.7002, Val Acc: 77.72%\n",
      "Epoch 6, Loss: 0.8591\n",
      "Validation Accuracy: 79.32%\n",
      "Epoch 6, LR: [0.001], Train Loss: 0.3288, Val Loss: 0.6553, Val Acc: 79.32%\n",
      "Epoch 7, Loss: 0.8972\n",
      "Validation Accuracy: 79.12%\n",
      "Epoch 7, LR: [0.001], Train Loss: 0.2604, Val Loss: 0.7013, Val Acc: 79.12%\n",
      "Epoch 8, Loss: 0.1480\n",
      "Validation Accuracy: 79.82%\n",
      "Epoch 8, LR: [0.001], Train Loss: 0.2141, Val Loss: 0.6872, Val Acc: 79.82%\n",
      "Epoch 9, Loss: 0.0718\n",
      "Validation Accuracy: 80.30%\n",
      "Best model updated and saved!\n",
      "Epoch 9, LR: [0.001], Train Loss: 0.1682, Val Loss: 0.7115, Val Acc: 80.30%\n",
      "Epoch 10, Loss: 0.1262\n",
      "Validation Accuracy: 80.22%\n",
      "Epoch 10, LR: [0.0001], Train Loss: 0.1516, Val Loss: 0.7517, Val Acc: 80.22%\n",
      "Epoch 11, Loss: 0.0263\n",
      "Validation Accuracy: 82.74%\n",
      "Best model updated and saved!\n",
      "Epoch 11, LR: [0.0001], Train Loss: 0.0534, Val Loss: 0.6919, Val Acc: 82.74%\n",
      "Epoch 12, Loss: 0.0053\n",
      "Validation Accuracy: 82.40%\n",
      "Epoch 12, LR: [0.0001], Train Loss: 0.0191, Val Loss: 0.7953, Val Acc: 82.40%\n",
      "Epoch 13, Loss: 0.0215\n",
      "Validation Accuracy: 82.42%\n",
      "Epoch 13, LR: [0.0001], Train Loss: 0.0097, Val Loss: 0.8589, Val Acc: 82.42%\n",
      "Epoch 14, Loss: 0.0024\n",
      "Validation Accuracy: 82.72%\n",
      "Epoch 14, LR: [0.0001], Train Loss: 0.0070, Val Loss: 0.9254, Val Acc: 82.72%\n",
      "Epoch 15, Loss: 0.0002\n",
      "Validation Accuracy: 82.44%\n",
      "Epoch 15, LR: [0.0001], Train Loss: 0.0042, Val Loss: 1.0128, Val Acc: 82.44%\n",
      "Epoch 16, Loss: 0.0009\n",
      "Validation Accuracy: 82.58%\n",
      "Epoch 16, LR: [0.0001], Train Loss: 0.0037, Val Loss: 1.0601, Val Acc: 82.58%\n",
      "Epoch 17, Loss: 0.0083\n",
      "Validation Accuracy: 82.56%\n",
      "Epoch 17, LR: [0.0001], Train Loss: 0.0037, Val Loss: 1.1173, Val Acc: 82.56%\n",
      "Epoch 18, Loss: 0.0002\n",
      "Validation Accuracy: 82.78%\n",
      "Best model updated and saved!\n",
      "Epoch 18, LR: [0.0001], Train Loss: 0.0031, Val Loss: 1.1549, Val Acc: 82.78%\n",
      "Epoch 19, Loss: 0.0000\n",
      "Validation Accuracy: 82.54%\n",
      "Epoch 19, LR: [0.0001], Train Loss: 0.0024, Val Loss: 1.1742, Val Acc: 82.54%\n",
      "Epoch 20, Loss: 0.0001\n",
      "Validation Accuracy: 82.94%\n",
      "Best model updated and saved!\n",
      "Epoch 20, LR: [1e-05], Train Loss: 0.0022, Val Loss: 1.1638, Val Acc: 82.94%\n",
      "Epoch 21, Loss: 0.0000\n",
      "Validation Accuracy: 82.88%\n",
      "Epoch 21, LR: [1e-05], Train Loss: 0.0018, Val Loss: 1.1690, Val Acc: 82.88%\n",
      "Epoch 22, Loss: 0.0014\n",
      "Validation Accuracy: 82.84%\n",
      "Epoch 22, LR: [1e-05], Train Loss: 0.0015, Val Loss: 1.1678, Val Acc: 82.84%\n",
      "Epoch 23, Loss: 0.0000\n",
      "Validation Accuracy: 82.66%\n",
      "Epoch 23, LR: [1e-05], Train Loss: 0.0012, Val Loss: 1.1849, Val Acc: 82.66%\n",
      "Epoch 24, Loss: 0.0001\n",
      "Validation Accuracy: 82.84%\n",
      "Epoch 24, LR: [1e-05], Train Loss: 0.0010, Val Loss: 1.1621, Val Acc: 82.84%\n",
      "Epoch 25, Loss: 0.0006\n",
      "Validation Accuracy: 82.40%\n",
      "Epoch 25, LR: [1e-05], Train Loss: 0.0009, Val Loss: 1.1920, Val Acc: 82.40%\n",
      "Epoch 26, Loss: 0.0000\n",
      "Validation Accuracy: 82.88%\n",
      "Epoch 26, LR: [1e-05], Train Loss: 0.0008, Val Loss: 1.1786, Val Acc: 82.88%\n",
      "Epoch 27, Loss: 0.0256\n",
      "Validation Accuracy: 82.94%\n",
      "Epoch 27, LR: [1e-05], Train Loss: 0.0007, Val Loss: 1.1921, Val Acc: 82.94%\n",
      "Epoch 28, Loss: 0.0000\n",
      "Validation Accuracy: 83.00%\n",
      "Best model updated and saved!\n",
      "Epoch 28, LR: [1e-05], Train Loss: 0.0005, Val Loss: 1.1936, Val Acc: 83.00%\n",
      "Epoch 29, Loss: 0.0004\n",
      "Validation Accuracy: 82.86%\n",
      "Epoch 29, LR: [1e-05], Train Loss: 0.0006, Val Loss: 1.2251, Val Acc: 82.86%\n",
      "Epoch 30, Loss: 0.0012\n",
      "Validation Accuracy: 82.82%\n",
      "Epoch 30, LR: [1.0000000000000002e-06], Train Loss: 0.0005, Val Loss: 1.2443, Val Acc: 82.82%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Make sure the checkpoints directory exists\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "best_acc = 0.0\n",
    "for epoch in range(num_epochs):  \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_data_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss+=loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            val_loss+=loss         \n",
    "    acc = 100 * correct / total\n",
    "    print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "    # --- Checkpointing ---\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"accuracy\": acc,\n",
    "        \"loss\": loss.item()\n",
    "    }\n",
    "    torch.save(checkpoint, f\"checkpoints/epoch_{epoch+1}.pth\")\n",
    "    \n",
    "    # Save best model separately\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        torch.save(model.state_dict(), \"checkpoints/best_model.pth\")\n",
    "        print(\"Best model updated and saved!\")\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step()        \n",
    "    print(f\"Epoch {epoch+1}, LR: {scheduler.get_last_lr()}, \"\n",
    "          f\"Train Loss: {running_loss/len(train_data_loader):.4f}, \"\n",
    "          f\"Val Loss: {val_loss/len(val_data_loader):.4f}, \"\n",
    "          f\"Val Acc: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa9cf81-8838-4022-83b2-79ad14f68d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
